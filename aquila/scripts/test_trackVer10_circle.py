#!/usr/bin/env python
# coding: utf-8

import os
import sys

# ==================== GPU Configuration ====================
os.environ['CUDA_VISIBLE_DEVICES'] = '1'
os.environ['XLA_FLAGS'] = '--xla_gpu_cuda_data_dir=/usr/local/cuda'

import time
import jax
import jax.numpy as jnp
import numpy as np
import pickle
import matplotlib.pyplot as plt
from matplotlib.patches import Rectangle
from mpl_toolkits.mplot3d import Axes3D

# Add parent directory to path for imports
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '../..'))

from aquila.envs.target_trackVer10 import TrackEnvVer10
from aquila.envs.wrappers import MinMaxObservationWrapper, NormalizeActionWrapper
from aquila.modules.mlp import MLP
from aquila.utils.trajectory_utils import (
    TrajectoryGenerator,
    CircularTrajectory,
    create_trajectory
)


def load_trained_policy(checkpoint_path):
    """åŠ è½½è®­ç»ƒå¥½çš„ç­–ç•¥å‚æ•°"""
    print(f"Loading policy from: {checkpoint_path}")
    
    with open(checkpoint_path, 'rb') as f:
        data = pickle.load(f)
    
    if isinstance(data, dict):
        params = data['params']
        env_config = data.get('env_config', {})
        final_loss = data.get('final_loss', 'Unknown')
        training_epochs = data.get('training_epochs', 'Unknown')
        action_repeat = data.get('action_repeat', 10)  # Ver10é»˜è®¤å€¼
        buffer_size = data.get('action_obs_buffer_size', 10)  # Ver10é»˜è®¤å€¼
    else:
        # å…¼å®¹æ—§æ ¼å¼
        params = data
        env_config = {}
        final_loss = 'Unknown'
        training_epochs = 'Unknown'
        action_repeat = 10  # Ver10é»˜è®¤å€¼
        buffer_size = 10  # Ver10é»˜è®¤å€¼
    
    print("âœ… Policy parameters loaded successfully!")
    print(f"   Final loss: {final_loss}")
    print(f"   Training epochs: {training_epochs}")
    print(f"   Action repeat: {action_repeat}")
    print(f"   Action-obs buffer size: {buffer_size}")
    
    return params, env_config, action_repeat, buffer_size


def ensure_trajectory_reasonable(trajectory: CircularTrajectory) -> CircularTrajectory:
    """
    ç¡®ä¿åœ†å½¢è½¨è¿¹å‚æ•°åˆç†ï¼ˆVer10æ²¡æœ‰è¾¹ç•Œçº¦æŸï¼Œåªéœ€ç¡®ä¿å‚æ•°åœ¨åˆç†èŒƒå›´å†…ï¼‰
    
    Args:
        trajectory: åŸå§‹åœ†å½¢è½¨è¿¹ç”Ÿæˆå™¨
        
    Returns:
        è°ƒæ•´åçš„è½¨è¿¹ç”Ÿæˆå™¨ï¼ˆæ–°å®ä¾‹ï¼Œä¸ä¿®æ”¹åŸå®ä¾‹ï¼‰
    """
    # Ver10æ²¡æœ‰è¾¹ç•Œçº¦æŸï¼Œåªéœ€ç¡®ä¿è½¨è¿¹å‚æ•°åˆç†
    # ç¡®ä¿åŠå¾„åœ¨åˆç†èŒƒå›´å†…
    adjusted_radius = np.clip(trajectory.radius, 0.5, 10.0)
    
    # ç¡®ä¿zåæ ‡åœ¨åˆç†èŒƒå›´å†…ï¼ˆNEDåæ ‡ç³»ï¼Œzé€šå¸¸ä¸ºè´Ÿå€¼ï¼‰
    center_z = np.clip(trajectory.center_z, -10.0, -0.5)
    
    # åˆ›å»ºæ–°çš„åœ†å½¢è½¨è¿¹
    return CircularTrajectory(
        center=(float(trajectory.center_x), float(trajectory.center_y), float(center_z)),
        radius=float(adjusted_radius),
        num_circles=trajectory.num_circles,
        ramp_up_time=trajectory.ramp_up_time,
        ramp_down_time=trajectory.ramp_down_time,
        circle_duration=trajectory.circle_duration,
        init_phase=trajectory.init_phase,
        max_speed=trajectory.max_speed
    )


def run_test_episode(env, policy_apply, params, key, action_repeat, buffer_size, 
                     trajectory: CircularTrajectory = None, verbose=False):
    """
    è¿è¡Œä¸€ä¸ªæµ‹è¯•episodeï¼Œè®°å½•è·Ÿè¸ªä¿¡æ¯ï¼ˆVer10æ²¡æœ‰è¾¹ç•Œçº¦æŸï¼‰
    
    Args:
        env: ç¯å¢ƒå®ä¾‹
        policy_apply: ç­–ç•¥åº”ç”¨å‡½æ•°
        params: ç­–ç•¥å‚æ•°
        key: JAXéšæœºæ•°ç”Ÿæˆå™¨å¯†é’¥
        action_repeat: åŠ¨ä½œé‡å¤æ¬¡æ•°
        buffer_size: åŠ¨ä½œ-è§‚æµ‹ç¼“å†²åŒºå¤§å°
        trajectory: åœ†å½¢è½¨è¿¹ç”Ÿæˆå™¨ï¼Œå¦‚æœæä¾›åˆ™è¦†ç›–ç¯å¢ƒçš„é»˜è®¤ç›®æ ‡è¿åŠ¨
        verbose: æ˜¯å¦æ‰“å°è¯¦ç»†ä¿¡æ¯
    """
    # Reset environmentï¼ˆå…ˆresetè·å–åŸºæœ¬çŠ¶æ€ç»“æ„ï¼‰
    state, obs = env.reset(key)
    
    # å¦‚æœæä¾›äº†è½¨è¿¹ç”Ÿæˆå™¨ï¼Œå…ˆç¡®ä¿è½¨è¿¹å‚æ•°åˆç†ï¼Œç„¶åæ ¹æ®è½¨è¿¹åˆå§‹åŒ–
    if trajectory is not None:
        # 1. ç¡®ä¿è½¨è¿¹å‚æ•°åˆç†ï¼ˆVer10æ²¡æœ‰è¾¹ç•Œçº¦æŸï¼‰
        trajectory = ensure_trajectory_reasonable(trajectory)
        
        # 2. è·å–è½¨è¿¹çš„åˆå§‹ä½ç½®ï¼ˆt=0æ—¶ï¼‰
        target_initial_pos, target_initial_vel = trajectory.get_state(0.0)
        target_initial_pos = np.array(target_initial_pos)  # è½¬æ¢ä¸ºnumpyä»¥ä¾¿è®¡ç®—
        target_initial_vel = jnp.array(target_initial_vel)
        
        # 3. æ ¹æ®ç›®æ ‡åˆå§‹ä½ç½®ï¼Œåˆå§‹åŒ–æ— äººæœºåœ¨ç›®æ ‡æ­£åæ–¹1må¤„ï¼ˆNEDåæ ‡ç³»ï¼Œ-Xæ–¹å‘ï¼‰
        # è¿™æ ·ç›®æ ‡å°±åœ¨æ— äººæœºæ­£å‰æ–¹1må¤„
        quad_initial_pos_np = target_initial_pos - np.array([1.0, 0.0, 0.0])  # æ­£åæ–¹1m
        quad_initial_pos = jnp.array(quad_initial_pos_np)
        target_initial_pos = jnp.array(target_initial_pos)
        
        # 4. æ›´æ–°stateä¸­çš„ç›®æ ‡ä½ç½®ã€é€Ÿåº¦å’Œæ— äººæœºä½ç½®
        import dataclasses
        from aquila.objects.quadrotor_obj import QuadrotorState
        
        # æ›´æ–°æ— äººæœºçŠ¶æ€ï¼ˆä¿æŒå…¶ä»–å±æ€§ä¸å˜ï¼Œåªæ›´æ–°ä½ç½®ï¼‰
        new_quadrotor_state = dataclasses.replace(
            state.quadrotor_state,
            p=quad_initial_pos
        )
        
        # æ›´æ–°æ•´ä¸ªstate
        state = dataclasses.replace(
            state,
            quadrotor_state=new_quadrotor_state,
            target_pos=target_initial_pos,
            target_vel=target_initial_vel
        )
        
        # 5. é‡æ–°è®¡ç®—è§‚æµ‹
        obs = env._get_obs(state)
    
    # åˆå§‹åŒ–åŠ¨ä½œ-çŠ¶æ€ç¼“å†²åŒº
    obs_dim = env.observation_space.shape[0]
    action_dim = env.action_space.shape[0]
    
    # âš ï¸ é‡è¦ï¼šç¼“å†²åŒºæ ¼å¼å¿…é¡»ä¸è®­ç»ƒæ—¶ä¸€è‡´ï¼š[action, obs]ï¼ˆå…ˆåŠ¨ä½œï¼Œåè§‚æµ‹ï¼‰
    # ç¼“å†²åŒºå½¢çŠ¶ï¼š(buffer_size, action_dim + obs_dim)
    action_obs_buffer = jnp.zeros((buffer_size, action_dim + obs_dim))
    
    # åˆå§‹åŒ–ï¼šå¡«å……é›¶åŠ¨ä½œå’Œé›¶è§‚æµ‹ï¼ˆä¸è®­ç»ƒæ—¶çš„åˆå§‹åŒ–ä¸€è‡´ï¼‰
    zero_action = jnp.zeros(action_dim)
    zero_obs = jnp.zeros(obs_dim)
    action_obs_combined = jnp.concatenate([zero_action, zero_obs])
    action_obs_buffer = jnp.tile(action_obs_combined[None, :], (buffer_size, 1))
    
    # Episode statistics
    episode_data = {
        'quad_positions': [],
        'target_positions': [],
        'distances': [],
        'rewards': [],
        'actions': [],
        'velocities': [],
        'terminated': False,
        'truncated': False,
        'num_steps': 0,
    }
    
    done = False
    step_count = 0
    action = jnp.zeros(action_dim)  # åˆå§‹åŠ¨ä½œä¸ºé›¶
    action_counter = 0  # åŠ¨ä½œè®¡æ•°å™¨ï¼Œç”¨äºaction_repeat
    
    # å¦‚æœæä¾›äº†è½¨è¿¹ç”Ÿæˆå™¨ï¼Œè®°å½•è½¨è¿¹èµ·å§‹æ—¶é—´
    trajectory_start_time = 0.0 if trajectory else None
    
    while not done and step_count < env.max_steps_in_episode:
        # æ¯action_repeatæ­¥è·å–ä¸€æ¬¡æ–°åŠ¨ä½œ
        if action_counter % action_repeat == 0:
            # âš ï¸ ä¸è®­ç»ƒæ—¶ä¸€è‡´çš„é€»è¾‘ï¼š
            # æ­¥éª¤1ï¼šå…ˆç”¨ç©ºåŠ¨ä½œ+å½“å‰è§‚æµ‹æ›´æ–°ç¼“å†²åŒºï¼ˆä¸ºè·å–æ–°åŠ¨ä½œåšå‡†å¤‡ï¼‰
            action_obs_buffer_for_input = jnp.roll(action_obs_buffer, shift=-1, axis=0)
            empty_action = jnp.zeros(action_dim)
            action_obs_combined_empty = jnp.concatenate([empty_action, obs])
            action_obs_buffer_for_input = action_obs_buffer_for_input.at[-1].set(action_obs_combined_empty)
            
            # æ­¥éª¤2ï¼šå±•å¹³ç¼“å†²åŒºä½œä¸ºç½‘ç»œè¾“å…¥ï¼š[action[0], obs[0], action[1], obs[1], ...]
            network_input = action_obs_buffer_for_input.flatten()
            
            # æ­¥éª¤3ï¼šè·å–æ–°åŠ¨ä½œ
            action = policy_apply(params, network_input)
            
            # æ­¥éª¤4ï¼šç”¨è·å–åˆ°çš„æ–°åŠ¨ä½œæ›´æ–°ç¼“å†²åŒºï¼ˆç”¨äºä¸‹æ¬¡ä½¿ç”¨ï¼‰
            action_obs_buffer = jnp.roll(action_obs_buffer, shift=-1, axis=0)
            action_obs_combined_new = jnp.concatenate([action, obs])
            action_obs_buffer = action_obs_buffer.at[-1].set(action_obs_combined_new)
        
        # æ‰§è¡ŒåŠ¨ä½œ
        key, subkey = jax.random.split(key)
        transition = env.step(state, action, subkey)
        next_state, next_obs, reward, terminated, truncated, info = transition
        
        # å¦‚æœæä¾›äº†è½¨è¿¹ç”Ÿæˆå™¨ï¼Œè¦†ç›–ç›®æ ‡ä½ç½®å’Œé€Ÿåº¦
        if trajectory is not None:
            current_time = trajectory_start_time + step_count * env.dt
            traj_pos, traj_vel = trajectory.get_state(current_time)
            
            # æ›´æ–°çŠ¶æ€ä¸­çš„ç›®æ ‡ä½ç½®å’Œé€Ÿåº¦ï¼ˆVer10æ²¡æœ‰è¾¹ç•Œçº¦æŸï¼‰
            import dataclasses
            next_state = dataclasses.replace(
                next_state,
                target_pos=traj_pos,
                target_vel=traj_vel
            )
            
            # é‡æ–°è®¡ç®—è§‚æµ‹ï¼ˆå› ä¸ºç›®æ ‡ä½ç½®æ”¹å˜äº†ï¼‰
            next_obs = env._get_obs(next_state)
        
        # è®°å½•æ•°æ®
        quad_pos = np.array(info['quad_p'])
        target_pos = np.array(info['target_p'])
        distance = np.array(info['distance_to_target'])
        
        episode_data['quad_positions'].append(quad_pos)
        episode_data['target_positions'].append(target_pos)
        episode_data['distances'].append(distance)
        episode_data['rewards'].append(float(reward))
        episode_data['actions'].append(np.array(action))
        episode_data['velocities'].append(np.array(info['quad_v']))
        episode_data['target_velocities'] = episode_data.get('target_velocities', [])
        episode_data['target_velocities'].append(np.array(info['target_v']))
        
        # æ›´æ–°çŠ¶æ€å’Œè§‚æµ‹
        state = next_state
        obs = next_obs  # æ›´æ–°obsä»¥ä¾¿ä¸‹æ¬¡è·å–åŠ¨ä½œæ—¶ä½¿ç”¨
        done = terminated or truncated
        step_count += 1
        action_counter += 1
        
        if verbose and step_count % 100 == 0:
            print(f"  Step {step_count}: Distance={distance:.3f}m, Reward={reward:.3f}")
    
    episode_data['terminated'] = bool(terminated)
    episode_data['truncated'] = bool(truncated)
    episode_data['num_steps'] = step_count
    
    return episode_data


def visualize_episode(episode_data, episode_idx=0, save_path=None):
    """å¯è§†åŒ–å•ä¸ªepisodeçš„ç»“æœï¼ˆVer10æ²¡æœ‰è¾¹ç•Œçº¦æŸï¼‰"""
    fig = plt.figure(figsize=(24, 16))
    
    # 1. 3Dè½¨è¿¹å›¾
    ax1 = fig.add_subplot(3, 3, 1, projection='3d')
    quad_positions = np.array(episode_data['quad_positions'])
    target_positions = np.array(episode_data['target_positions'])
    
    ax1.plot(quad_positions[:, 0], quad_positions[:, 1], quad_positions[:, 2], 
             'b-', label='Quadrotor', linewidth=2, alpha=0.7)
    ax1.plot(target_positions[:, 0], target_positions[:, 1], target_positions[:, 2], 
             'r--', label='Target', linewidth=2, alpha=0.7)
    
    ax1.set_xlabel('X (North) [m]')
    ax1.set_ylabel('Y (East) [m]')
    ax1.set_zlabel('Z (Down) [m]')
    ax1.set_title(f'Episode {episode_idx}: 3D Trajectory (NED frame)')
    ax1.legend()
    ax1.grid(True)
    
    # è®¾ç½®åæ ‡è½´èŒƒå›´ï¼ˆåŸºäºå®é™…è½¨è¿¹èŒƒå›´ï¼‰
    all_pos = np.vstack([quad_positions, target_positions])
    x_range = [all_pos[:, 0].min() - 1.0, all_pos[:, 0].max() + 1.0]
    y_range = [all_pos[:, 1].min() - 1.0, all_pos[:, 1].max() + 1.0]
    z_range = [all_pos[:, 2].min() - 1.0, all_pos[:, 2].max() + 1.0]
    ax1.set_xlim(x_range)
    ax1.set_ylim(y_range)
    ax1.set_zlim(z_range)
    
    # 2. XYå¹³é¢æŠ•å½±ï¼ˆä¿¯è§†å›¾ï¼‰
    ax2 = fig.add_subplot(3, 3, 2)
    ax2.plot(quad_positions[:, 0], quad_positions[:, 1], 'b-', label='Quadrotor', linewidth=2, alpha=0.7)
    ax2.plot(target_positions[:, 0], target_positions[:, 1], 'r--', label='Target', linewidth=2, alpha=0.7)
    
    ax2.set_xlabel('X (North) [m]')
    ax2.set_ylabel('Y (East) [m]')
    ax2.set_title(f'Episode {episode_idx}: Top View (XY plane)')
    ax2.legend()
    ax2.grid(True)
    ax2.axis('equal')
    
    # 3. è·Ÿè¸ªè·ç¦»éšæ—¶é—´å˜åŒ–
    ax3 = fig.add_subplot(3, 3, 3)
    distances = np.array(episode_data['distances'])
    ax3.plot(distances, 'b-', linewidth=2)
    ax3.axhline(y=1.0, color='r', linestyle='--', label='Target Distance (1m)')
    ax3.axhline(y=1.3, color='orange', linestyle=':', label='Acceptable Range (Â±30cm)')
    ax3.axhline(y=0.7, color='orange', linestyle=':')
    ax3.set_xlabel('Step')
    ax3.set_ylabel('Distance to Target [m]')
    ax3.set_title(f'Episode {episode_idx}: Tracking Distance')
    ax3.legend()
    ax3.grid(True)
    
    # 4. XZå¹³é¢æŠ•å½±ï¼ˆä¾§è§†å›¾ï¼‰
    ax4 = fig.add_subplot(3, 3, 4)
    ax4.plot(quad_positions[:, 0], quad_positions[:, 2], 'b-', label='Quadrotor', linewidth=2, alpha=0.7)
    ax4.plot(target_positions[:, 0], target_positions[:, 2], 'r--', label='Target', linewidth=2, alpha=0.7)
    ax4.set_xlabel('X (North) [m]')
    ax4.set_ylabel('Z (Down) [m]')
    ax4.set_title(f'Episode {episode_idx}: Side View (XZ plane)')
    ax4.legend()
    ax4.grid(True)
    ax4.axis('equal')
    
    # 5. å¥–åŠ±éšæ—¶é—´å˜åŒ–
    ax5 = fig.add_subplot(3, 3, 5)
    rewards = np.array(episode_data['rewards'])
    ax5.plot(rewards, 'purple', linewidth=2)
    ax5.set_xlabel('Step')
    ax5.set_ylabel('Reward')
    ax5.set_title(f'Episode {episode_idx}: Rewards')
    ax5.grid(True)
    
    # 6. ç›®æ ‡ç‰©ä½“é€Ÿåº¦éšæ—¶é—´å˜åŒ–
    ax6 = fig.add_subplot(3, 3, 6)
    target_velocities = np.array(episode_data.get('target_velocities', []))
    if len(target_velocities) > 0:
        target_speed = np.linalg.norm(target_velocities, axis=1)
        ax6.plot(target_speed, 'r-', linewidth=2, label='Target Speed')
        ax6.axhline(y=1.0, color='orange', linestyle='--', linewidth=1, label='Max Speed (1 m/s)')
        ax6.set_xlabel('Step')
        ax6.set_ylabel('Speed [m/s]')
        ax6.set_title(f'Episode {episode_idx}: Target Speed')
        ax6.legend()
        ax6.grid(True)
        ax6.set_ylim(bottom=0)
    
    # 7. æ— äººæœºé€Ÿåº¦éšæ—¶é—´å˜åŒ–
    ax7 = fig.add_subplot(3, 3, 7)
    velocities = np.array(episode_data['velocities'])
    quad_speed = np.linalg.norm(velocities, axis=1)
    ax7.plot(quad_speed, 'b-', linewidth=2, label='Quad Speed')
    ax7.set_xlabel('Step')
    ax7.set_ylabel('Speed [m/s]')
    ax7.set_title(f'Episode {episode_idx}: Quadrotor Speed')
    ax7.legend()
    ax7.grid(True)
    ax7.set_ylim(bottom=0)
    
    # 8. ç›®æ ‡é€Ÿåº¦å‘é‡ï¼ˆ3ä¸ªåˆ†é‡ï¼‰
    ax8 = fig.add_subplot(3, 3, 8)
    if len(target_velocities) > 0:
        ax8.plot(target_velocities[:, 0], 'r-', alpha=0.7, label='Vx (North)', linewidth=1.5)
        ax8.plot(target_velocities[:, 1], 'g-', alpha=0.7, label='Vy (East)', linewidth=1.5)
        ax8.plot(target_velocities[:, 2], 'b-', alpha=0.7, label='Vz (Down)', linewidth=1.5)
        ax8.set_xlabel('Step')
        ax8.set_ylabel('Velocity [m/s]')
        ax8.set_title(f'Episode {episode_idx}: Target Velocity Components')
        ax8.legend()
        ax8.grid(True)
    
    # 9. ç»Ÿè®¡ä¿¡æ¯æ–‡æœ¬
    ax9 = fig.add_subplot(3, 3, 9)
    ax9.axis('off')
    
    mean_distance = np.mean(distances)
    std_distance = np.std(distances)
    min_distance = np.min(distances)
    max_distance = np.max(distances)
    mean_reward = np.mean(rewards)
    total_reward = np.sum(rewards)
    
    # è®¡ç®—è·Ÿè¸ªæˆåŠŸç‡ï¼ˆè·ç¦»åœ¨ç›®æ ‡Â±30cmå†…çš„æ¯”ä¾‹ï¼‰
    tracking_success_rate = np.mean(np.abs(distances - 1.0) < 0.3) * 100
    
    # è®¡ç®—ç›®æ ‡é€Ÿåº¦ç»Ÿè®¡
    target_velocities = np.array(episode_data.get('target_velocities', []))
    if len(target_velocities) > 0:
        target_speed = np.linalg.norm(target_velocities, axis=1)
        mean_target_speed = np.mean(target_speed)
        max_target_speed = np.max(target_speed)
    else:
        mean_target_speed = 0.0
        max_target_speed = 0.0
    
    stats_text = f"""
    Episode {episode_idx} Statistics:
    
    Steps: {episode_data['num_steps']}
    Terminated: {episode_data['terminated']}
    Truncated: {episode_data['truncated']}
    
    Tracking Performance:
    â€¢ Mean Distance: {mean_distance:.3f} m
    â€¢ Std Distance: {std_distance:.3f} m
    â€¢ Min Distance: {min_distance:.3f} m
    â€¢ Max Distance: {max_distance:.3f} m
    â€¢ Success Rate (Â±30cm): {tracking_success_rate:.1f}%
    
    Target Motion:
    â€¢ Mean Speed: {mean_target_speed:.3f} m/s
    â€¢ Max Speed: {max_target_speed:.3f} m/s
    
    Rewards:
    â€¢ Mean Reward: {mean_reward:.3f}
    â€¢ Total Reward: {total_reward:.3f}
    """
    
    ax9.text(0.1, 0.5, stats_text, fontsize=10, verticalalignment='center',
             family='monospace', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.3))
    
    plt.tight_layout()
    
    if save_path:
        plt.savefig(save_path, dpi=150, bbox_inches='tight')
        print(f"  Visualization saved to: {save_path}")
    
    return fig


def main():
    # ==================== Configuration ====================
    print(f"JAX devices: {jax.devices()}")
    print(f"JAX device count: {jax.device_count()}")
    
    # ==================== Circular Trajectory Setup ====================
    # åˆ›å»ºå±…ä¸­çš„åœ†å½¢è½¨è¿¹
    # å‚æ•°ä¼šåœ¨ç¯å¢ƒåˆ›å»ºåæ ¹æ®è¾¹ç•Œè¿›è¡Œè°ƒæ•´
    trajectory = create_trajectory(
        'circular',
        center=(0.0, 0.0, -2.0),  # ä¸´æ—¶ä¸­å¿ƒï¼Œä¼šæ ¹æ®è¾¹ç•Œè°ƒæ•´
        radius=2.0,                # åŠå¾„ï¼ˆç±³ï¼‰ï¼Œä¼šæ ¹æ®è¾¹ç•Œè°ƒæ•´
        num_circles=2,             # åœ†åœˆæ•°é‡
        ramp_up_time=3.0,          # åŠ é€Ÿæ—¶é—´ï¼ˆç§’ï¼‰
        ramp_down_time=3.0,        # å‡é€Ÿæ—¶é—´ï¼ˆç§’ï¼‰
        circle_duration=20.0,      # å•åœˆåä¹‰æŒç»­æ—¶é—´ï¼ˆç§’ï¼‰
        init_phase=0.0,            # åˆå§‹ç›¸ä½ï¼ˆå¼§åº¦ï¼‰
        max_speed=1.0              # æœ€å¤§é€Ÿåº¦é™åˆ¶ï¼ˆm/sï¼‰
    )
    
    print(f"\n{'='*60}")
    print(f"Circular Trajectory Configuration:")
    print(f"{'='*60}")
    traj_info = trajectory.get_info()
    for key, value in traj_info.items():
        print(f"  {key}: {value}")
    print(f"Note: Trajectory parameters will be adjusted to reasonable ranges")
    print(f"{'='*60}\n")
    
    # Load trained policy
    policy_file = 'aquila/param/trackVer10_policy.pkl'
    
    if not os.path.exists(policy_file):
        print(f"âŒ Error: Policy file not found: {policy_file}")
        print("   Please train the model first using train_trackVer10.py")
        return
    
    params, env_config, action_repeat, buffer_size = load_trained_policy(policy_file)
    
    # ==================== Environment Setup ====================
    # Create env with same configuration as training (Ver10æ²¡æœ‰è¾¹ç•Œçº¦æŸ)
    env = TrackEnvVer10(
        max_steps_in_episode=env_config.get('max_steps_in_episode', 1000),
        dt=env_config.get('dt', 0.01),
        delay=env_config.get('delay', 0.03),
        omega_std=0.1,
        action_penalty_weight=env_config.get('action_penalty_weight', 0.5),
        target_height=env_config.get('target_height', 2.0),
        target_init_distance_min=env_config.get('target_init_distance_min', 0.5),
        target_init_distance_max=env_config.get('target_init_distance_max', 1.5),
        target_speed_max=env_config.get('target_speed_max', 1.0),
        reset_distance=env_config.get('reset_distance', 100.0),
        max_speed=env_config.get('max_speed', 20.0),
        thrust_to_weight_min=env_config.get('thrust_to_weight_min', 1.2),
        thrust_to_weight_max=env_config.get('thrust_to_weight_max', 5.0),
        disturbance_mag=0.0,  # æµ‹è¯•æ—¶å…³é—­æ‰°åŠ¨
    )
    
    # Apply wrappers
    env = MinMaxObservationWrapper(env)
    env = NormalizeActionWrapper(env)
    
    # ==================== Model Setup ====================
    obs_dim = env.observation_space.shape[0]
    action_dim = env.action_space.shape[0]
    input_dim = buffer_size * (obs_dim + action_dim)
    
    policy = MLP([input_dim, 128, 128, action_dim], initial_scale=0.2)
    
    print(f"\n{'='*60}")
    print(f"Test Configuration:")
    print(f"{'='*60}")
    print(f"Environment: TrackEnvVer10 (no boundary constraints)")
    print(f"Observation dimension: {obs_dim}")
    print(f"Action dimension: {action_dim}")
    print(f"Action repeat: {action_repeat}")
    print(f"Action-obs buffer size: {buffer_size}")
    print(f"Input dimension: {input_dim}")
    print(f"Disturbance: DISABLED (for testing)")
    print(f"{'='*60}\n")
    
    # ==================== Run Test Episodes ====================
    num_test_episodes = 1
    print(f"Running {num_test_episodes} test episode...\n")
    
    key = jax.random.key(42)  # ä½¿ç”¨å›ºå®šç§å­ä»¥ä¾¿å¤ç°
    
    all_episode_data = []
    
    for episode_idx in range(num_test_episodes):
        print(f"Episode {episode_idx + 1}/{num_test_episodes}:")
        key, subkey = jax.random.split(key)
        
        episode_data = run_test_episode(
            env, policy.apply, params, subkey, 
            action_repeat, buffer_size, 
            trajectory=trajectory,  # ä¼ é€’è½¨è¿¹ç”Ÿæˆå™¨
            verbose=True
        )
        
        all_episode_data.append(episode_data)
        
        # Print episode summary
        mean_distance = np.mean(episode_data['distances'])
        tracking_success_rate = np.mean(np.abs(np.array(episode_data['distances']) - 1.0) < 0.3) * 100
        
        print(f"  âœ“ Completed {episode_data['num_steps']} steps")
        print(f"    Mean tracking distance: {mean_distance:.3f}m")
        print(f"    Tracking success rate (Â±30cm): {tracking_success_rate:.1f}%")
        print(f"    Terminated: {episode_data['terminated']}")
        print()
    
    # ==================== Aggregate Statistics ====================
    print(f"\n{'='*60}")
    print(f"Test Results:")
    print(f"{'='*60}\n")
    
    # Tracking performance
    all_distances = np.concatenate([np.array(ep['distances']) for ep in all_episode_data])
    mean_distance_all = np.mean(all_distances)
    std_distance_all = np.std(all_distances)
    tracking_success_rate_all = np.mean(np.abs(all_distances - 1.0) < 0.3) * 100
    
    print("ğŸ“Š Tracking Performance:")
    print(f"   â€¢ Mean distance to target: {mean_distance_all:.3f} Â± {std_distance_all:.3f} m")
    print(f"   â€¢ Success rate (Â±30cm from 1m): {tracking_success_rate_all:.1f}%")
    print(f"   â€¢ Min distance: {np.min(all_distances):.3f} m")
    print(f"   â€¢ Max distance: {np.max(all_distances):.3f} m")
    
    # Termination statistics
    total_steps = sum(ep['num_steps'] for ep in all_episode_data)
    num_terminated = sum(1 for ep in all_episode_data if ep['terminated'])
    num_truncated = sum(1 for ep in all_episode_data if ep['truncated'])
    
    print(f"\nğŸ“ˆ Episode Statistics:")
    print(f"   â€¢ Episode terminated early: {num_terminated} (tracking lost)")
    print(f"   â€¢ Episode completed fully: {num_truncated} (reached max steps)")
    print(f"   â€¢ Episode length: {total_steps} steps")
    
    # Overall assessment
    print(f"\n{'='*60}")
    print(f"Overall Assessment:")
    print(f"{'='*60}")
    
    tracking_passed = tracking_success_rate_all >= 70  # è‡³å°‘70%çš„æ—¶é—´åœ¨Â±30cmå†…
    
    print(f"âœ“ Tracking Test: {'PASSED âœ…' if tracking_passed else 'FAILED âŒ'}")
    print(f"  (Success rate {tracking_success_rate_all:.1f}% {'â‰¥' if tracking_passed else '<'} 70%)")
    
    if tracking_passed:
        print(f"\nğŸ‰ Tracking test PASSED! The policy successfully tracks the circular trajectory target.")
    else:
        print(f"\nâš ï¸  Tracking test FAILED. The policy needs further training or tuning.")
    
    # ==================== Visualization ====================
    print(f"\n{'='*60}")
    print(f"Generating visualizations...")
    print(f"{'='*60}\n")
    
    # Create output directory
    output_dir = 'aquila/output/trackVer10'
    os.makedirs(output_dir, exist_ok=True)
    
    # Visualize episode
    print(f"Visualizing episode...")
    save_path = f'{output_dir}/episode_1.png'
    fig = visualize_episode(
        all_episode_data[0], 
        episode_idx=1,
        save_path=save_path
    )
    plt.close(fig)
    
    # Create summary plot
    print("Creating summary plot...")
    fig, axes = plt.subplots(2, 2, figsize=(16, 12))
    
    # 1. Distance distribution
    ax = axes[0, 0]
    ax.hist(all_distances, bins=50, alpha=0.7, color='blue', edgecolor='black')
    ax.axvline(x=1.0, color='r', linestyle='--', linewidth=2, label='Target (1m)')
    ax.axvline(x=0.7, color='orange', linestyle=':', linewidth=2, label='Acceptable Range')
    ax.axvline(x=1.3, color='orange', linestyle=':', linewidth=2)
    ax.set_xlabel('Distance to Target [m]')
    ax.set_ylabel('Frequency')
    ax.set_title('Distribution of Tracking Distances')
    ax.legend()
    ax.grid(True, alpha=0.3)
    
    # 2. Tracking distance over time
    ax = axes[0, 1]
    distances = np.array(all_episode_data[0]['distances'])
    ax.plot(distances, 'b-', linewidth=2, alpha=0.7)
    ax.axhline(y=1.0, color='r', linestyle='--', linewidth=2, label='Target (1m)')
    ax.axhline(y=1.3, color='orange', linestyle=':', linewidth=1, label='Acceptable Range')
    ax.axhline(y=0.7, color='orange', linestyle=':', linewidth=1)
    ax.set_xlabel('Step')
    ax.set_ylabel('Distance to Target [m]')
    ax.set_title('Tracking Distance Over Time')
    ax.legend()
    ax.grid(True, alpha=0.3)
    
    # 3. Reward over time
    ax = axes[1, 0]
    rewards = np.array(all_episode_data[0]['rewards'])
    ax.plot(rewards, 'purple', linewidth=2, alpha=0.7)
    ax.set_xlabel('Step')
    ax.set_ylabel('Reward')
    ax.set_title('Reward Over Time')
    ax.grid(True, alpha=0.3)
    
    # 4. Target speed over time
    ax = axes[1, 1]
    target_velocities = np.array(all_episode_data[0].get('target_velocities', []))
    if len(target_velocities) > 0:
        target_speed = np.linalg.norm(target_velocities, axis=1)
        ax.plot(target_speed, 'r-', linewidth=2, alpha=0.7, label='Target Speed')
        ax.axhline(y=1.0, color='orange', linestyle='--', linewidth=1, label='Max Speed (1 m/s)')
        ax.set_xlabel('Step')
        ax.set_ylabel('Speed [m/s]')
        ax.set_title('Target Speed Over Time')
        ax.legend()
        ax.grid(True, alpha=0.3)
        ax.set_ylim(bottom=0)
    
    plt.tight_layout()
    summary_path = f'{output_dir}/summary.png'
    plt.savefig(summary_path, dpi=150, bbox_inches='tight')
    print(f"  Summary plot saved to: {summary_path}")
    plt.close(fig)
    
    print(f"\nâœ… All visualizations saved to: {output_dir}/")
    print(f"\nTest completed! ğŸ‰")


if __name__ == "__main__":
    main()

