#!/usr/bin/env python
# coding: utf-8

import os
import sys

# ==================== GPU Configuration ====================
# å¿…é¡»åœ¨å¯¼å…¥JAXä¹‹å‰è®¾ç½®CUDA_VISIBLE_DEVICES
os.environ['CUDA_VISIBLE_DEVICES'] = '0'  # ä½¿ç”¨å•å¼ GPU
os.environ['XLA_FLAGS'] = '--xla_gpu_cuda_data_dir=/usr/local/cuda'

import time
import math
import jax
import jax.numpy as jnp
import matplotlib.pyplot as plt
import numpy as np
import optax
from flax.training.train_state import TrainState
import pickle
from torch.utils.tensorboard import SummaryWriter
from flax import linen as nn

# Add parent directory to path for imports
# aquila/scripts/train_trackVer14.py -> ../../ -> Aquila project root
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '../..'))

from aquila.envs.target_trackVer16 import TrackEnvVer16  # ä½¿ç”¨TrackEnvVer16ï¼ˆåŸºäºVer15ï¼Œæ·»åŠ åœ†å½¢è½¨è¿¹æ”¯æŒï¼‰
from aquila.envs.wrappers import MinMaxObservationWrapper, NormalizeActionWrapper
from aquila.modules.mlp import MLP
from aquila.algos import bpttVer3

"""
TrackVer16: åŸºäºVer15ï¼Œæ·»åŠ åœ†å½¢è½¨è¿¹æ”¯æŒå’ŒéªŒè¯æœºåˆ¶
- action_repeat = 2 (æ¯2ä¸ªstepæ‰è·å–ä¸€æ¬¡æ–°åŠ¨ä½œï¼Œæ¯ç§’50æ¬¡åŠ¨ä½œï¼Œæ¯æ¬¡æŒç»­0.02ç§’)
- buffer_size = 50 (åŠ¨ä½œ-çŠ¶æ€ç¼“å†²åŒºå¤§å°)
- ç½‘ç»œè¾“å‡ºï¼šåŠ¨ä½œ (4,) + è¾…åŠ©è¾“å‡º (3,) = ç›®æ ‡é€Ÿåº¦é¢„æµ‹ï¼ˆæœºä½“ç³»ï¼‰
- è¾…åŠ©æŸå¤±ï¼šé¢„æµ‹å€¼ä¸çœŸå®å€¼çš„L2è·ç¦»ï¼Œæƒé‡ä¸º1.0
- éªŒè¯ï¼šæ¯10ä¸ªepochä½¿ç”¨åœ†å½¢è½¨è¿¹éªŒè¯ä¸€æ¬¡
- ä¿å­˜ï¼šè®­ç»ƒæœ€ç»ˆæƒé‡ + éªŒè¯é›†ä¸Šæœ€ä½³æƒé‡
"""


def load_trained_policy(checkpoint_path):
    """åŠ è½½è®­ç»ƒå¥½çš„ç­–ç•¥å‚æ•°"""
    print(f"Loading policy from: {checkpoint_path}")
    
    with open(checkpoint_path, 'rb') as f:
        data = pickle.load(f)
    
    if isinstance(data, dict):
        params = data['params']
        env_config = data.get('env_config', {})
        final_loss = data.get('final_loss', 'Unknown')
        training_epochs = data.get('training_epochs', 'Unknown')
    else:
        # å…¼å®¹æ—§æ ¼å¼
        params = data
        env_config = {}
        final_loss = 'Unknown'
        training_epochs = 'Unknown'
    
    print("âœ… Policy parameters loaded successfully!")
    print(f"   Final loss: {final_loss}")
    print(f"   Training epochs: {training_epochs}")
    
    return params, env_config


def main():
    # ==================== GPU Configuration Info ====================
    print(f"JAX devices: {jax.devices()}")
    print(f"JAX device count: {jax.device_count()}")
    
    # ==================== Environment Setup ====================
    # Create env - ä½¿ç”¨TrackEnvVer16ç¯å¢ƒï¼ˆåŸºäºVer15ï¼Œæ·»åŠ åœ†å½¢è½¨è¿¹æ”¯æŒï¼‰ï¼Œè®­ç»ƒ50Hzç½‘ç»œ
    env = TrackEnvVer16(
        max_steps_in_episode=600,  # è¿½è¸ªä»»åŠ¡çš„æœ€å¤§æ­¥æ•°
        dt=0.01,  # ä½¿ç”¨å®Œæ•´å››æ—‹ç¿¼çš„é»˜è®¤æ—¶é—´æ­¥é•¿
        delay=0.03,  # å¯é€‰æ‰§è¡Œå»¶è¿Ÿ
        omega_std=0.1,
        action_penalty_weight=0.5,
        # Tracking specific parameters
        target_height=2.0,  # m (é«˜åº¦2ç±³ï¼Œå®é™…ä¼šåœ¨(-2.2, -1.8)èŒƒå›´å†…éšæœº)
        target_init_distance_min=0.5,  # m (xè½´ä¸Šçš„åˆå§‹è·ç¦»æœ€å°å€¼)
        target_init_distance_max=1.5,  # m (xè½´ä¸Šçš„åˆå§‹è·ç¦»æœ€å¤§å€¼)
        target_speed_max=3.0,  # m/s (ç›®æ ‡æœ€å¤§é€Ÿåº¦ä¸Šé™ï¼Œå®é™…æ¯episodeä¼šåœ¨0-1ä¹‹é—´éšæœº)
        target_acceleration_max=5,  # m/sÂ² (ç›®æ ‡æœ€å¤§åŠ é€Ÿåº¦ï¼Œæ¯æ¬¡resetæ—¶åœ¨0åˆ°æ­¤å€¼ä¹‹é—´éšæœºç”Ÿæˆå®é™…åŠ é€Ÿåº¦)
        reset_distance=100.0,  # m (é‡ç½®è·ç¦»é˜ˆå€¼)
        max_speed=20.0,  # m/s
        # Parameter randomization (quadrotor)
        thrust_to_weight_min=1.2,  # æœ€å°æ¨é‡æ¯”
        thrust_to_weight_max=5.0,  # æœ€å¤§æ¨é‡æ¯”
        disturbance_mag=2.0,  # è®­ç»ƒæ—¶å¼€å¯å¸¸å€¼éšæœºæ‰°åŠ¨ï¼ˆ2Nï¼‰ï¼Œæé«˜é²æ£’æ€§
    )
    
    # Normalize obs to [-1,1] and actions to [-1,1]
    env = MinMaxObservationWrapper(env)
    env = NormalizeActionWrapper(env)
    
    # ==================== Model Setup ====================
    # è¾“å…¥ç»´åº¦å˜ä¸ºç¼“å†²åŒºå¤§å° * (è§‚æµ‹ç»´åº¦ + åŠ¨ä½œç»´åº¦)
    buffer_size = 50  # åŠ¨ä½œ-çŠ¶æ€ç¼“å†²åŒºå¤§å°
    action_dim = env.action_space.shape[0]
    obs_dim = env.observation_space.shape[0]
    input_dim = buffer_size * (obs_dim + action_dim)  # è¾“å…¥ç»´åº¦ä¸ºç¼“å†²åŒºå¤§å°ä¹˜ä»¥(è§‚æµ‹ç»´åº¦+åŠ¨ä½œç»´åº¦)
    
    # åˆ›å»ºç­–ç•¥ç½‘ç»œï¼Œè¾“å‡º7ç»´ï¼šaction (4,) + aux_output (3,)
    # ä½¿ç”¨MLPç›´æ¥è¾“å‡º7ç»´ï¼Œç„¶ååœ¨bpttVer3ä¸­åˆ†å‰²æˆ (action, aux_output)
    output_dim = action_dim + 3  # 4ä¸ªåŠ¨ä½œ + 3ä¸ªè¾…åŠ©è¾“å‡º
    policy = MLP([input_dim, 128, 128, output_dim], initial_scale=0.2)
    
    print(f"Observation dimension: {obs_dim}")
    print(f"Action dimension: {action_dim}")
    print(f"Action-obs buffer size: {buffer_size}")
    print(f"Input dimension (buffer_size * (obs_dim + action_dim)): {input_dim}")
    print(f"Network output: {output_dim}ç»´ (action {action_dim} + aux_output 3)")
    
    # ==================== Training Parameters ====================
    num_epochs = 300  # è®­ç»ƒè½®æ•°
    num_envs = 512 # å¹¶è¡Œç¯å¢ƒæ•°ï¼ˆå•å¡ï¼‰
    
    # åŠ¨ä½œé‡å¤å‚æ•°
    action_repeat = 2  # æ¯2ä¸ªstepæ‰è·å–ä¸€æ¬¡æ–°åŠ¨ä½œï¼ˆæ¯ç§’50æ¬¡åŠ¨ä½œï¼Œæ¯æ¬¡æŒç»­0.02ç§’ï¼‰
    
    # Ver16: éªŒè¯å‚æ•°
    validation_interval = 10  # æ¯10ä¸ªepochéªŒè¯ä¸€æ¬¡
    num_val_episodes = 3  # éªŒè¯æ—¶è¿è¡Œçš„episodeæ•°é‡ï¼ˆä»10æ”¹åˆ°3ï¼ŒåŠ å¿«éªŒè¯é€Ÿåº¦ï¼‰
    val_max_steps = 600  # éªŒè¯æ—¶æ¯ä¸ªepisodeçš„æœ€å¤§æ­¥æ•°ï¼ˆä»600æ”¹åˆ°300ï¼‰
    
    # Optimizer - ä½¿ç”¨ä½™å¼¦è¡°å‡å­¦ä¹ ç‡
    initial_learning_rate = 5e-3
    end_learning_rate = 5e-3
    scheduler = optax.cosine_decay_schedule(
        init_value=initial_learning_rate,
        decay_steps=num_epochs,
        alpha=end_learning_rate/initial_learning_rate
    )
    tx = optax.chain(
        optax.clip_by_global_norm(1.0),  # æ¢¯åº¦è£å‰ª
        optax.adam(scheduler)
    )
    
    # Init params
    key = jax.random.key(0)
    init_params = policy.initialize(key)
    
    # ==================== Choose Training Mode ====================
    choice = 1   # 1: ä½¿ç”¨åˆå§‹å‚æ•°, 2: ä½¿ç”¨åŠ è½½çš„å‚æ•°
    
    if choice == 1:
        # ä½¿ç”¨åˆå§‹å‚æ•°
        train_state = TrainState.create(
            apply_fn=policy.apply,
            params=init_params,
            tx=tx
        )
        print("âœ… ä½¿ç”¨åˆå§‹ç½‘ç»œå‚æ•°å¼€å§‹è®­ç»ƒ")
    else:
        # ä½¿ç”¨åŠ è½½çš„å‚æ•°
        policy_file = 'aquila/param_saved/trackVer16_policy.pkl'  # ä½¿ç”¨Ver16çš„æ¨¡å‹æ–‡ä»¶
        loaded_params, env_config = load_trained_policy(policy_file)
        train_state = TrainState.create(
            apply_fn=policy.apply,
            params=loaded_params,
            tx=tx
        )
        print("âœ… ä½¿ç”¨åŠ è½½çš„ç½‘ç»œå‚æ•°ç»§ç»­è®­ç»ƒ")
    
    # ==================== TensorBoard Setup ====================
    # åˆ›å»ºtensorboardæ—¥å¿—ç›®å½•
    log_dir = f'runs/trackVer16_{time.strftime("%Y%m%d_%H%M%S")}'  # ä½¿ç”¨Ver16çš„æ—¥å¿—ç›®å½•
    writer = SummaryWriter(log_dir)
    print(f"TensorBoard logs will be saved to: {log_dir}")
    print(f"Run 'tensorboard --logdir=runs' to view training progress")
    
    # è®¾ç½® tensorboard writer åˆ° bpttVer3 æ¨¡å—ï¼ˆç”¨äºå®æ—¶è®°å½•ï¼‰
    bpttVer3.set_tensorboard_writer(writer)
    
    # ==================== Validation Setup ====================
    # Ver16: åˆ›å»ºå•ç‹¬çš„éªŒè¯ç¯å¢ƒï¼ˆä½¿ç”¨åœ†å½¢è½¨è¿¹ï¼Œå‚è€ƒRealFlightçš„target_publisher_nodeï¼‰
    # åœ†å½¢è½¨è¿¹å‚æ•°ï¼ˆå‚è€ƒtarget_publisher_node.cppçš„é»˜è®¤å‚æ•°ï¼‰
    # ENUåæ ‡ç³»è½¬NEDï¼šcircle_center_z=1.2 -> -1.2ï¼Œä½†å®é™…ä½¿ç”¨-2æ›´åˆé€‚
    circle_radius = 2.0  # åŠå¾„2ç±³
    circle_duration = 20.0  # åœ†å½¢è¿åŠ¨å‘¨æœŸ20ç§’
    circle_angular_vel = 2.0 * jnp.pi / circle_duration  # è§’é€Ÿåº¦
    circle_init_phase = 0.0  # åˆå§‹ç›¸ä½
    circle_center = jnp.array([0.0, 0.0, -2.0])  # åœ†å¿ƒä½ç½®ï¼ˆNEDåæ ‡ç³»ï¼‰
    
    # åˆ›å»ºéªŒè¯ç¯å¢ƒï¼ˆä¸è®­ç»ƒç¯å¢ƒç›¸åŒçš„é…ç½®ï¼Œä½†ç”¨äºéªŒè¯ï¼‰
    val_env_raw = TrackEnvVer16(
        max_steps_in_episode=600,
        dt=0.01,
        delay=0.03,
        omega_std=0.1,
        action_penalty_weight=0.5,
        target_height=2.0,
        target_init_distance_min=0.5,
        target_init_distance_max=1.5,
        target_speed_max=3.0,
        target_acceleration_max=5,
        reset_distance=100.0,
        max_speed=20.0,
        thrust_to_weight_min=1.2,
        thrust_to_weight_max=5.0,
        disturbance_mag=0.0,  # éªŒè¯æ—¶ä¸æ·»åŠ æ‰°åŠ¨
    )
    # å¯¹éªŒè¯ç¯å¢ƒåº”ç”¨ç›¸åŒçš„åŒ…è£…
    val_env = MinMaxObservationWrapper(val_env_raw)
    val_env = NormalizeActionWrapper(val_env)
    
    # è·å–è§‚æµ‹ç©ºé—´çš„è¾¹ç•Œç”¨äºæ‰‹åŠ¨å½’ä¸€åŒ–
    val_obs_min = jnp.array(val_env_raw.observation_space.low)
    val_obs_max = jnp.array(val_env_raw.observation_space.high)
    
    def normalize_obs(obs):
        """æ‰‹åŠ¨å½’ä¸€åŒ–è§‚æµ‹åˆ° [-1, 1]"""
        return 2.0 * (obs - val_obs_min) / (val_obs_max - val_obs_min) - 1.0
    
    # è®°å½•éªŒè¯æœ€ä½³æ€§èƒ½
    best_val_loss = float('inf')
    best_val_params = None
    best_val_epoch = 0
    
    def validation_fn(current_train_state, epoch_idx):
        """éªŒè¯å‡½æ•°ï¼šä½¿ç”¨åœ†å½¢è½¨è¿¹æµ‹è¯•ç­–ç•¥æ€§èƒ½"""
        nonlocal best_val_loss, best_val_params, best_val_epoch
        
        val_losses = []
        val_distances = []
        val_velocities = []
        
        # åˆ›å»ºéªŒè¯ç”¨çš„éšæœºkey
        val_key = jax.random.key(epoch_idx + 10000)
        
        # è¿è¡Œå¤šä¸ªéªŒè¯episode
        for ep_idx in range(num_val_episodes):
            val_key, episode_key = jax.random.split(val_key)
            
            # é‡ç½®éªŒè¯ç¯å¢ƒï¼ˆä½¿ç”¨åœ†å½¢è½¨è¿¹ï¼‰
            # ç›´æ¥è®¿é—®åŸå§‹ç¯å¢ƒæ¥ä½¿ç”¨æ–°çš„resetå‚æ•°
            val_state, val_obs_raw = val_env_raw.reset(
                episode_key, 
                None, 
                None,
                use_circular_trajectory=True,
                circular_center=circle_center,
                circular_radius=circle_radius,
                circular_angular_vel=circle_angular_vel,
                circular_init_phase=circle_init_phase
            )
            
            # æ‰‹åŠ¨å½’ä¸€åŒ–è§‚æµ‹
            val_obs = normalize_obs(val_obs_raw)
            
            episode_reward = 0.0
            episode_distances = []
            episode_velocities = []
            
            # åˆå§‹åŒ–åŠ¨ä½œç¼“å†²åŒºï¼ˆä¸è®­ç»ƒæ—¶ç›¸åŒï¼‰
            action_obs_buffer = jnp.zeros((1, buffer_size, obs_dim + action_dim))
            
            # è¿è¡Œä¸€ä¸ªepisodeï¼ˆé™åˆ¶æœ€å¤§æ­¥æ•°ï¼‰
            for step_idx in range(val_max_steps):
                # æ›´æ–°ç¼“å†²åŒºï¼ˆæ·»åŠ ç©ºåŠ¨ä½œ + å½“å‰è§‚æµ‹ï¼‰
                action_obs_buffer = jnp.roll(action_obs_buffer, shift=-1, axis=1)
                empty_action = jnp.zeros((1, 4))
                action_obs_combined = jnp.concatenate([empty_action, val_obs[None, :]], axis=1)
                action_obs_buffer = action_obs_buffer.at[:, -1, :].set(action_obs_combined)
                
                # è·å–åŠ¨ä½œï¼ˆæ¯action_repeatæ­¥ï¼‰
                if step_idx % action_repeat == 0:
                    action_obs_buffer_flat = action_obs_buffer.reshape(1, -1)
                    output_7d = current_train_state.apply_fn(current_train_state.params, action_obs_buffer_flat)
                    action = output_7d[:, :4]  # å‰4ç»´ï¼šåŠ¨ä½œ
                    aux_output = output_7d[:, 4:]  # å3ç»´ï¼šè¾…åŠ©è¾“å‡º
                    
                    # æ›´æ–°ç¼“å†²åŒºï¼ˆç”¨çœŸå®åŠ¨ä½œæ›¿æ¢ç©ºåŠ¨ä½œï¼‰
                    action_obs_buffer = jnp.roll(action_obs_buffer, shift=-1, axis=1)
                    action_obs_combined = jnp.concatenate([action, val_obs[None, :]], axis=1)
                    action_obs_buffer = action_obs_buffer.at[:, -1, :].set(action_obs_combined)
                
                # æ‰§è¡ŒåŠ¨ä½œï¼ˆç›´æ¥åœ¨åŸå§‹ç¯å¢ƒä¸Šï¼‰
                val_key, step_key = jax.random.split(val_key)
                transition = val_env_raw.step(val_state, (action[0], aux_output[0]), step_key)
                val_state, val_obs_raw, reward, terminated, truncated, info = transition
                
                # æ‰‹åŠ¨å½’ä¸€åŒ–è§‚æµ‹
                val_obs = normalize_obs(val_obs_raw)
                
                episode_reward += reward
                episode_distances.append(float(info['distance_to_target']))
                episode_velocities.append(float(jnp.linalg.norm(info['quad_v'])))
                
                if terminated or truncated:
                    break
            
            val_losses.append(-float(episode_reward) / (step_idx + 1))
            val_distances.append(np.mean(episode_distances))
            val_velocities.append(np.mean(episode_velocities))
        
        # è®¡ç®—å¹³å‡æŒ‡æ ‡
        avg_val_loss = np.mean(val_losses)
        avg_val_distance = np.mean(val_distances)
        avg_val_velocity = np.mean(val_velocities)
        std_val_loss = np.std(val_losses)
        
        # è®°å½•åˆ°tensorboard
        writer.add_scalar('Validation/loss', avg_val_loss, epoch_idx)
        writer.add_scalar('Validation/distance', avg_val_distance, epoch_idx)
        writer.add_scalar('Validation/velocity', avg_val_velocity, epoch_idx)
        writer.add_scalar('Validation/loss_std', std_val_loss, epoch_idx)
        
        # æ£€æŸ¥æ˜¯å¦ä¸ºæœ€ä½³éªŒè¯æ€§èƒ½
        if avg_val_loss < best_val_loss:
            best_val_loss = avg_val_loss
            best_val_params = current_train_state.params
            best_val_epoch = epoch_idx
            writer.add_scalar('Validation/best_loss', best_val_loss, epoch_idx)
            print(f"  ğŸ¯ æ–°çš„æœ€ä½³éªŒè¯æ€§èƒ½ï¼Loss: {best_val_loss:.6f}")
        
        return {
            'val_loss': avg_val_loss,
            'val_distance': avg_val_distance,
            'val_velocity': avg_val_velocity,
            'val_loss_std': std_val_loss,
        }
    
    # ==================== Training ====================
    time_start = time.time()
    training_log = []
    
    print(f"\n{'='*60}")
    print(f"å¼€å§‹è®­ç»ƒè¿½è¸ªä»»åŠ¡ (TrackVer16 - åŸºäºVer15ï¼Œæ·»åŠ åœ†å½¢è½¨è¿¹æ”¯æŒå’ŒéªŒè¯æœºåˆ¶)...")
    print(f"Total environments: {num_envs}")
    print(f"Number of epochs: {num_epochs}")
    print(f"Steps per epoch: {env.max_steps_in_episode}")
    print(f"Action repeat: {action_repeat} steps")
    print(f"Action-obs buffer size: {buffer_size}")
    print(f"Input dimension: {input_dim}")
    print(f"Network output: action + aux_target_vel (è¾…åŠ©æŸå¤±)")
    print(f"Validation interval: {validation_interval} epochs")
    print(f"Validation episodes: {num_val_episodes}")
    print(f"Quadrotor model: Full (Quadrotor - based on agilicious framework)")
    print(f"{'='*60}\n")
    
    res_dict = bpttVer3.train(
        env=env,
        train_state=train_state,
        num_epochs=num_epochs,
        num_steps_per_epoch=env.max_steps_in_episode,
        num_envs=num_envs,
        key=key,
        truncate_k=500,  # 500è¡¨ç¤ºå®Œæ•´BPTT
        action_repeat=action_repeat,  # ä¼ é€’åŠ¨ä½œé‡å¤å‚æ•°
        buffer_size=buffer_size,  # ä¼ é€’åŠ¨ä½œ-çŠ¶æ€ç¼“å†²åŒºå¤§å°å‚æ•°
        validation_fn=validation_fn,  # Ver16: ä¼ é€’éªŒè¯å‡½æ•°
        validation_interval=validation_interval,  # Ver16: ä¼ é€’éªŒè¯é—´éš”
    )
    
    print("\nâœ… è®­ç»ƒå®Œæˆï¼å¼€å§‹å¤„ç†ç»“æœ...")
    sys.stdout.flush()
    
    time_end = time.time()
    training_time = time_end - time_start
    
    # ==================== Record Training Results ====================
    # è·å–æ‰€æœ‰epochçš„æŸå¤±
    losses = res_dict["metrics"]  # shape: (num_epochs,)
    losses_np = np.array(losses)
    
    # è¡¥å……è®°å½•æ¯ä¸ªepochçš„æŸå¤±åˆ°tensorboardï¼ˆå¡«è¡¥å®æ—¶è®°å½•çš„é—´éš™ï¼‰
    print("\nè¡¥å……è®°å½•è®­ç»ƒæ•°æ®åˆ° TensorBoard...")
    for epoch_idx in range(num_epochs):
        loss_value = float(losses_np[epoch_idx])
        # è¿™é‡Œä¼šè¦†ç›–ä¹‹å‰å®æ—¶è®°å½•çš„å€¼ï¼Œä½†æ²¡å…³ç³»ï¼Œæ•°æ®æ˜¯ä¸€è‡´çš„
        writer.add_scalar('Loss/train_complete', loss_value, epoch_idx)
        training_log.append(loss_value)
        
        # æ¯100ä¸ªepochæ‰“å°ä¸€æ¬¡
        if (epoch_idx + 1) % 100 == 0:
            print(f"Epoch {epoch_idx + 1}/{num_epochs}, Loss: {loss_value:.6f}")
    
    
    # è®°å½•æŸå¤±ç»Ÿè®¡ä¿¡æ¯
    writer.add_scalar('Loss/initial', float(losses_np[0]), 0)
    writer.add_scalar('Loss/final', float(losses_np[-1]), 0)
    writer.add_scalar('Loss/min', float(np.min(losses_np)), 0)
    writer.add_scalar('Loss/max', float(np.max(losses_np)), 0)
    writer.add_scalar('Loss/mean', float(np.mean(losses_np)), 0)
    writer.add_scalar('Loss/std', float(np.std(losses_np)), 0)
    
    # è®°å½•åŠ¨ä½œ-çŠ¶æ€ç¼“å†²åŒºç›¸å…³çš„ç»Ÿè®¡ä¿¡æ¯
    writer.add_scalar('Config/action_repeat', action_repeat, 0)
    writer.add_scalar('Config/action_obs_buffer_size', buffer_size, 0)
    writer.add_scalar('Config/input_dimension', input_dim, 0)
    writer.add_scalar('Config/effective_actions_per_epoch', env.max_steps_in_episode / action_repeat, 0)
    writer.add_scalar('Config/auxiliary_loss', 1.0, 0)  # Ver16ç»§æ‰¿ï¼šè¾…åŠ©æŸå¤±æƒé‡
    writer.add_scalar('Config/validation_interval', validation_interval, 0)  # Ver16æ–°å¢ï¼šéªŒè¯é—´éš”
    
    # è·å–æ›´æ–°åçš„è®­ç»ƒçŠ¶æ€
    train_state = res_dict["runner_state"].train_state
    final_loss = float(losses_np[-1])
    
    # ==================== Print Summary ====================
    print(f"\n{'='*60}")
    print(f"è¿½è¸ªä»»åŠ¡è®­ç»ƒå®Œæˆï¼(TrackVer16 - åŸºäºVer15ï¼Œæ·»åŠ åœ†å½¢è½¨è¿¹æ”¯æŒå’ŒéªŒè¯æœºåˆ¶)")
    print(f"{'='*60}")
    print(f"Training time: {training_time:.2f} seconds ({training_time/60:.2f} minutes)")
    print(f"Final Loss: {final_loss:.6f}")
    print(f"Initial Loss: {float(losses_np[0]):.6f}")
    print(f"Loss improvement: {float(losses_np[0] - losses_np[-1]):.6f}")
    print(f"Best validation loss: {best_val_loss:.6f} (epoch {best_val_epoch})")
    print(f"Action repeat: {action_repeat} steps")
    print(f"Action-obs buffer size: {buffer_size}")
    print(f"Input dimension: {input_dim}")
    print(f"Effective actions per epoch: {env.max_steps_in_episode / action_repeat:.1f}")
    print(f"Network output: action + aux_target_vel (è¾…åŠ©æŸå¤±)")
    print(f"Quadrotor model: Full (Quadrotor - based on agilicious framework)")
    
    
    # ==================== Save Model ====================
    # ä¿å­˜æœ€ç»ˆè®­ç»ƒæƒé‡
    checkpoint_data = {
        'params': train_state.params,
        'training_log': training_log,
        'final_loss': final_loss,
        'num_epochs': num_epochs,
        'training_time': training_time,
        'action_repeat': action_repeat,  # ä¿å­˜åŠ¨ä½œé‡å¤å‚æ•°
        'action_obs_buffer_size': buffer_size,  # ä¿å­˜åŠ¨ä½œ-çŠ¶æ€ç¼“å†²åŒºå¤§å°å‚æ•°
        'input_dimension': input_dim,  # ä¿å­˜è¾“å…¥ç»´åº¦å‚æ•°
        'best_val_loss': best_val_loss,  # Ver16: ä¿å­˜æœ€ä½³éªŒè¯æŸå¤±
        'best_val_epoch': best_val_epoch,  # Ver16: ä¿å­˜æœ€ä½³éªŒè¯epoch
        'env_config': {
            'max_steps_in_episode': env.max_steps_in_episode,
            'dt': env.dt,
            'delay': env.delay,
            'action_penalty_weight': env.action_penalty_weight,
            # Ver16 åŸºäºVer15ï¼Œæ·»åŠ åœ†å½¢è½¨è¿¹æ”¯æŒå’ŒéªŒè¯æœºåˆ¶
            'target_height': env.target_height,
            'target_init_distance_min': env.target_init_distance_min,
            'target_init_distance_max': env.target_init_distance_max,
            'target_speed_max': env.target_speed_max,
            'target_acceleration_max': env.target_acceleration_max,
            'reset_distance': env.reset_distance,
            'max_speed': env.max_speed,
        }
    }
    
    # ç¡®ä¿ç›®å½•å­˜åœ¨
    os.makedirs('aquila/param_saved', exist_ok=True)
    
    # ä¿å­˜æœ€ç»ˆæƒé‡ä¸ºpickleæ–‡ä»¶
    checkpoint_path = 'aquila/param_saved/trackVer16_policy_final.pkl'  # ä½¿ç”¨Ver16çš„æ–‡ä»¶å
    with open(checkpoint_path, 'wb') as f:
        pickle.dump(checkpoint_data, f)
    print(f"\nâœ… Final training policy saved as: {checkpoint_path}")
    
    # Ver16: ä¿å­˜æœ€ä½³éªŒè¯æƒé‡
    if best_val_params is not None:
        best_checkpoint_data = {
            'params': best_val_params,
            'training_log': training_log,
            'final_loss': final_loss,
            'best_val_loss': best_val_loss,
            'best_val_epoch': best_val_epoch,
            'num_epochs': num_epochs,
            'training_time': training_time,
            'action_repeat': action_repeat,
            'action_obs_buffer_size': buffer_size,
            'input_dimension': input_dim,
            'env_config': checkpoint_data['env_config']
        }
        
        best_checkpoint_path = 'aquila/param_saved/trackVer16_policy_best_val.pkl'
        with open(best_checkpoint_path, 'wb') as f:
            pickle.dump(best_checkpoint_data, f)
        print(f"âœ… Best validation policy saved as: {best_checkpoint_path}")
    
    # é¢å¤–ä¿å­˜ä¸€ä¸ªå¸¦æ—¶é—´æˆ³çš„å¤‡ä»½
    timestamp = time.strftime("%Y%m%d_%H%M%S")
    backup_path = f'aquila/param_saved/trackVer16_policy_{timestamp}.pkl'  # ä½¿ç”¨Ver16çš„æ–‡ä»¶å
    with open(backup_path, 'wb') as f:
        pickle.dump(checkpoint_data, f)
    print(f"âœ… Backup saved as: {backup_path}")
    
    # å…³é—­tensorboard writer
    writer.close()
    print(f"\nâœ… TensorBoard logs saved to: {log_dir}")
    print(f"   Run 'tensorboard --logdir=runs' to view the results")


if __name__ == "__main__":
    main()